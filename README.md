# ğŸ§  RAG Chatbot Application

A full-stack Retrieval-Augmented Generation (RAG) chatbot application that leverages a **React + Vite** frontend, a **Python** backend, and a **self-hosted Ollama** LLM to deliver contextual, high-performance AI conversations enhanced by custom knowledge sources.

---

## ğŸ”§ Tech Stack

- ğŸ§© **Frontend**: React + Vite  
- ğŸ **Backend**: Python (FastAPI/Flask/Django â€” customize as needed)  
- ğŸ§  **LLM**: Self-hosted Ollama  
- ğŸ“„ **RAG Pipeline**: Embedding-based retrieval system  
- ğŸ—ƒï¸ **Vector Store**: (e.g., FAISS, Chroma, Weaviate â€” customize if used)  

---

## ğŸš€ Features

- ğŸ” **Contextual Search**: Augments chatbot responses using embedded documents  
- ğŸ’¬ **Natural Conversation**: Chat interface powered by Ollama LLM  
- ğŸ“š **Knowledge Injection**: Bring your own docs and PDFs  
- âš™ï¸ **Modular Architecture**: Frontend and backend are loosely coupled  
- ğŸ” **Local-Only Option**: Entire pipeline can run offline with self-hosted LLM  

---


---

## ğŸ› ï¸ Setup Instructions

### 1. ğŸ§  Set Up Ollama Locally

Install and run Ollama:

```bash
# Install Ollama
brew install ollama

# Run your desired model (example: llama3)
ollama run llama3

```
---


### 2. ğŸ§  Set Up Python backend

cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run backend (adjust if using FastAPI/Flask/etc.)
uvicorn server.main:app --reload --port 8000

---



### 3. âš›ï¸ Set Up React + Vite Frontend
cd frontend
npm install
npm run dev

Frontend will run at http://localhost:5173.

---


## Sample API call

``` bash 
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "How to open demat account?"}'
  ```

---


## To-Do / Enhancements

 Add document uploader (PDF, TXT)

 Implement user chat history persistence

 Add authentication (optional)

 Dockerize for full deployment

---

## Contact
Feel free to reach out for feedback or collaborations!
